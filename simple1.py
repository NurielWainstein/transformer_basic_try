import torch
import numpy as np
import math
import torch
import torch.nn as nn

from torch.utils.data import Dataset, DataLoader
from torch import nn

torch.set_printoptions(threshold=10_000)

data = [('01/10', '01/01/10'), ('02/10', '01/02/10'), ('03/10', '01/03/10'), ('04/10', '01/04/10'), ('05/10', '01/05/10'), ('06/10', '01/06/10'), ('07/10', '01/07/10'), ('08/10', '01/08/10'), ('09/10', '01/09/10'), ('10/10', '01/10/10'), ('11/10', '01/11/10'), ('12/10', '01/12/10'), ('01/11', '01/01/11'), ('02/11', '01/02/11'), ('03/11', '01/03/11'), ('04/11', '01/04/11'), ('05/11', '01/05/11'), ('06/11', '01/06/11'), ('07/11', '01/07/11'), ('08/11', '01/08/11'), ('09/11', '01/09/11'), ('10/11', '01/10/11'), ('11/11', '01/11/11'), ('12/11', '01/12/11'), ('01/12', '01/01/12'), ('02/12', '01/02/12'), ('03/12', '01/03/12'), ('04/12', '01/04/12'), ('05/12', '01/05/12'), ('06/12', '01/06/12'), ('07/12', '01/07/12'), ('08/12', '01/08/12'), ('09/12', '01/09/12'), ('10/12', '01/10/12'), ('11/12', '01/11/12'), ('12/12', '01/12/12'), ('01/13', '01/01/13'), ('02/13', '01/02/13'), ('03/13', '01/03/13'), ('04/13', '01/04/13'), ('05/13', '01/05/13'), ('06/13', '01/06/13'), ('07/13', '01/07/13'), ('08/13', '01/08/13'), ('09/13', '01/09/13'), ('10/13', '01/10/13'), ('11/13', '01/11/13'), ('12/13', '01/12/13'), ('01/14', '01/01/14'), ('02/14', '01/02/14'), ('03/14', '01/03/14'), ('05/14', '01/05/14'), ('06/14', '01/06/14'), ('07/14', '01/07/14'), ('08/14', '01/08/14'), ('09/14', '01/09/14'), ('10/14', '01/10/14'), ('11/14', '01/11/14'), ('12/14', '01/12/14'), ('01/15', '01/01/15'), ('02/15', '01/02/15'), ('03/15', '01/03/15'), ('04/15', '01/04/15'), ('05/15', '01/05/15'), ('06/15', '01/06/15'), ('07/15', '01/07/15'), ('08/15', '01/08/15'), ('09/15', '01/09/15'), ('10/15', '01/10/15'), ('11/15', '01/11/15'), ('12/15', '01/12/15'), ('01/16', '01/01/16'), ('02/16', '01/02/16'), ('03/16', '01/03/16'), ('04/16', '01/04/16'), ('05/16', '01/05/16'), ('06/16', '01/06/16'), ('07/16', '01/07/16'), ('08/16', '01/08/16'), ('09/16', '01/09/16'), ('10/16', '01/10/16'), ('11/16', '01/11/16'), ('12/16', '01/12/16'), ('01/17', '01/01/17'), ('02/17', '01/02/17'), ('03/17', '01/03/17'), ('04/17', '01/04/17'), ('05/17', '01/05/17'), ('06/17', '01/06/17'), ('07/17', '01/07/17'), ('08/17', '01/08/17'), ('09/17', '01/09/17'), ('10/17', '01/10/17'), ('11/17', '01/11/17'), ('12/17', '01/12/17'), ('01/18', '01/01/18'), ('02/18', '01/02/18'), ('03/18', '01/03/18'), ('04/18', '01/04/18'), ('05/18', '01/05/18'), ('06/18', '01/06/18'), ('07/18', '01/07/18'), ('08/18', '01/08/18'), ('09/18', '01/09/18'), ('10/18', '01/10/18'), ('11/18', '01/11/18'), ('12/18', '01/12/18'), ('01/19', '01/01/19'), ('03/19', '01/03/19'), ('04/19', '01/04/19'), ('05/19', '01/05/19'), ('06/19', '01/06/19'), ('07/19', '01/07/19'), ('08/19', '01/08/19'), ('09/19', '01/09/19'), ('10/19', '01/10/19'), ('11/19', '01/11/19'), ('12/19', '01/12/19'), ('01/20', '01/01/20'), ('02/20', '01/02/20'), ('03/20', '01/03/20'), ('04/20', '01/04/20'), ('05/20', '01/05/20'), ('06/20', '01/06/20'), ('07/20', '01/07/20'), ('08/20', '01/08/20'), ('09/20', '01/09/20'), ('10/20', '01/10/20'), ('11/20', '01/11/20'), ('12/20', '01/12/20'), ('01/21', '01/01/21'), ('02/21', '01/02/21'), ('03/21', '01/03/21'), ('04/21', '01/04/21'), ('05/21', '01/05/21'), ('06/21', '01/06/21'), ('07/21', '01/07/21'), ('08/21', '01/08/21'), ('09/21', '01/09/21'), ('10/21', '01/10/21'), ('11/21', '01/11/21'), ('12/21', '01/12/21'), ('01/22', '01/01/22'), ('02/22', '01/02/22'), ('03/22', '01/03/22'), ('04/22', '01/04/22'), ('05/22', '01/05/22'), ('06/22', '01/06/22'), ('07/22', '01/07/22'), ('08/22', '01/08/22'), ('09/22', '01/09/22'), ('10/22', '01/10/22'), ('11/22', '01/11/22'), ('12/22', '01/12/22'), ('01/23', '01/01/23'), ('02/23', '01/02/23'), ('03/23', '01/03/23'), ('04/23', '01/04/23'), ('05/23', '01/05/23'), ('07/23', '01/07/23'), ('08/23', '01/08/23'), ('09/23', '01/09/23'), ('10/23', '01/10/23'), ('11/23', '01/11/23'), ('12/23', '01/12/23'), ('01/24', '01/01/24'), ('02/24', '01/02/24'), ('03/24', '01/03/24'), ('04/24', '01/04/24'), ('05/24', '01/05/24'), ('06/24', '01/06/24'), ('07/24', '01/07/24'), ('08/24', '01/08/24'), ('09/24', '01/09/24'), ('10/24', '01/10/24'), ('11/24', '01/11/24'), ('12/24', '01/12/24'), ('01/25', '01/01/25'), ('02/25', '01/02/25'), ('03/25', '01/03/25'), ('04/25', '01/04/25'), ('05/25', '01/05/25'), ('06/25', '01/06/25'), ('07/25', '01/07/25'), ('08/25', '01/08/25'), ('09/25', '01/09/25'), ('10/25', '01/10/25'), ('11/25', '01/11/25'), ('12/25', '01/12/25'), ('01/26', '01/01/26'), ('02/26', '01/02/26'), ('03/26', '01/03/26'), ('04/26', '01/04/26'), ('05/26', '01/05/26'), ('06/26', '01/06/26'), ('07/26', '01/07/26'), ('08/26', '01/08/26'), ('09/26', '01/09/26'), ('10/26', '01/10/26'), ('11/26', '01/11/26'), ('12/26', '01/12/26'), ('01/27', '01/01/27'), ('02/27', '01/02/27'), ('03/27', '01/03/27'), ('04/27', '01/04/27'), ('05/27', '01/05/27'), ('06/27', '01/06/27'), ('07/27', '01/07/27'), ('08/27', '01/08/27'), ('09/27', '01/09/27'), ('10/27', '01/10/27'), ('11/27', '01/11/27'), ('12/27', '01/12/27'), ('01/28', '01/01/28'), ('03/28', '01/03/28'), ('04/28', '01/04/28'), ('05/28', '01/05/28'), ('06/28', '01/06/28'), ('07/28', '01/07/28'), ('08/28', '01/08/28'), ('09/28', '01/09/28'), ('10/28', '01/10/28'), ('11/28', '01/11/28'), ('12/28', '01/12/28'), ('01/29', '01/01/29'), ('02/29', '01/02/29'), ('03/29', '01/03/29'), ('04/29', '01/04/29'), ('05/29', '01/05/29'), ('06/29', '01/06/29'), ('07/29', '01/07/29'), ('08/29', '01/08/29'), ('09/29', '01/09/29'), ('10/29', '01/10/29'), ('11/29', '01/11/29'), ('12/29', '01/12/29'), ('01/30', '01/01/30'), ('02/30', '01/02/30'), ('03/30', '01/03/30'), ('04/30', '01/04/30'), ('05/30', '01/05/30'), ('06/30', '01/06/30'), ('07/30', '01/07/30'), ('08/30', '01/08/30'), ('09/30', '01/09/30'), ('10/30', '01/10/30'), ('11/30', '01/11/30')]

index = 0
my_dict = {}

for tup in data:
    for date_str in tup:
        date_parts = date_str.split('/')
        for part in date_parts:
            if part not in my_dict:
                tensor = torch.zeros(32)
                tensor[index] = 1
                my_dict[part] = tensor
                index += 1
for char in "/":
    tensor = torch.zeros(32)
    tensor[index] = 1
    my_dict[char] = tensor
tensor = torch.zeros(32)
tensor[index+1] = 1
my_dict["<pad>"] = tensor


def split_list(input_list):
    n = len(input_list)
    split_index = int(n * 0.8)
    trainig_data = input_list[:split_index]
    testing_data = input_list[split_index:]
    return trainig_data, testing_data


trainig_data, testing_data = split_list(data)

def parse_data(trainig_data):
    t_data = []
    for tup in trainig_data:
        current = []

        for element in tup[:2]:
            element_with_spaces = element.replace('/', ' / ')
            words = element_with_spaces.split()
            sentence_tensor = torch.zeros((32, 32))
            for i in range(32):
                sentence_tensor[i] = my_dict["<pad>"]
            for j, word in enumerate(words):
                word_tensor = my_dict[word]
                sentence_tensor[j] = word_tensor

            current.append(sentence_tensor)

        t_data.append((current[0], current[1]))
    return t_data

test_data = parse_data(testing_data)
t_data = parse_data(trainig_data)

class MyDataset(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        input_value = self.data[idx][0]
        expected_output = self.data[idx][1]
        return input_value, expected_output


my_dataset = MyDataset(t_data)
batch_size = 4
my_dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True)


class TransformerModel(nn.Module):
    def __init__(self, vocab_size=32, d_model=32, nhead=4, num_layers=6):
        super(TransformerModel, self).__init__()
        self.d_model = d_model
        self.transformer_encoder = nn.TransformerEncoder(
            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=0.2),
            num_layers=num_layers)
        self.model = nn.Transformer(d_model=d_model, nhead=nhead)
        self.softmax = nn.Softmax(dim=2)

    def forward(self, src):
        out_encoder = self.transformer_encoder(src)
        out_transformer = self.model(src, out_encoder)

        # Apply a linear layer to get the output logits
        out_encoder = self.softmax(out_transformer)
        return out_encoder


model = TransformerModel()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
criterion = nn.L1Loss()

def fix_batches(tensor):
    x1, x2, x3, x4 = torch.split(tensor, 1, dim=0)
    return torch.cat((x1, x2, x3, x4), dim=0)

for epoch in range(5):
    for i, batch in enumerate(my_dataloader):
        inputs, expected_outputs = batch
        if inputs.size() == (1,32,32):
            continue
        else:
            inputs = (fix_batches(inputs)).squeeze()
            expected_outputs = (fix_batches(expected_outputs)).squeeze()
        # forward pass
        outputs = model(inputs)
        loss = criterion(outputs, expected_outputs)

        # backward pass
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        # print progress
        if (i + 1) % 1 == 0:
            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.10f}'
                  .format(epoch + 1, 10, i + 1, len(my_dataloader), loss.item()))


test_string = test_data[1][0].unsqueeze(0)
final_string = ""
with torch.no_grad():
    output = model.forward(test_string)
    output = output.argmax(-1).squeeze()

    for el in output:
        tensor = torch.zeros(32)
        tensor[el.item()] = 1
        for key, value in my_dict.items():
            equal = torch.eq(value, tensor)
            if equal.all():
                final_string = final_string + key
    print(final_string)

